{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continual Learning and Infinite width limits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are intrested in uderstanding the origin and the behaviour of Catastrophic forgetting in the Continual Learning framework.\n",
    "\n",
    "To obtain better insights on it, the infinite width limit of the model gives us deterministic dynamics, due to concetration of measures given by the Central Limit Theorem.\n",
    "\n",
    "The main deterministic quantity to observe and study is the so called Neural Tangent Kernel, which can be defined as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $$K_t^{NTK}(x,x') = \\sum_{\\theta}\\frac{\\partial f(x,\\theta_t)}{\\partial \\theta} \\cdot \\frac{\\partial f(x',\\theta_t)}{\\partial \\theta} \\in \\mathcal{R}^{P \\times P}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where:\n",
    "* P are the number of samples in the batch\n",
    "* $\\theta$ the network's parameter\n",
    "* $f(x,\\theta)$ is the network's output given input $x$ and the configuration $\\theta$ of the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parametrisations\n",
    "We can define two ways to initialise the network, given a factor $1/\\gamma$ applied to the output layer, and the learning rate:\n",
    "* Mean Field or Maximum Update Parametrisation ($\\mu P$): $\\gamma = \\mathcal{O}(\\sqrt{N}), \\eta = \\mathcal{O}(\\gamma^2)$\n",
    "* Neural Tangent Parametrisation (NTP): $\\gamma = \\mathcal{O}(1), \\eta = \\mathcal{O}(1)$\n",
    "with N the network widths.\n",
    "\n",
    "From basic assumpions on the grandient dynamics and the use of L2 loss, we can get the evolution of the residuals $\\Delta = y - f(x,\\theta)$:\n",
    "####  $$ \\frac{\\partial \\Delta(t)}{\\partial t} = - K^{NTK}(t)\\Delta(t) $$\n",
    "\n",
    "In the NTP, the NTK is fixed at initialisation for a infinitely wide network, and this allows us to obtain a closed form solution for the residuals evolution:\n",
    "####  $$ \\frac{\\partial \\Delta(t)}{\\partial t} = - K^{NTK}\\Delta(t) \\quad \\rightarrow \\quad \\Delta(t) = \\Delta_0 \\exp(- {K^{NTK}t})$$ \n",
    "\n",
    "In the $\\mu P$ the NTK is free to evolve and thus move from initialisation. To keep track of its evolution a Dynamical Mean Field Theory approach is essential. We create 2 fields: \n",
    "* a forward field $h_\\mu^\\ell(t)$ that represents population of the hidden representations of input $x_\\mu$ in each layer $\\ell$, that progates from the input layer to the output one\n",
    "* a backward one $g_\\mu^\\ell(t)$, that represents the populations of the gradients updates, relative to the input $x_\\mu$, at each layer $\\ell$,, that starts from the output and finishes at input.\n",
    "\n",
    "Thank to these 2 fields we can build the NTK as follows:\n",
    "#### $$ K ^{NTK}_{\\mu \\alpha}(t,t)=\\sum_{l=0}^{L} G_{\\mu \\alpha}^{l+1}(t, t) \\Phi_{\\mu \\alpha}^{l}(t, t)  $$\n",
    "where $\\Phi_{\\mu \\alpha}^\\ell(t, t) = \\langle \\phi( h_\\mu^\\ell(t)) \\cdot \\phi(h_\\alpha^\\ell(t) )\\rangle $ and $G_{\\mu \\alpha}^\\ell(t,t) = \\langle g_\\mu^\\ell(t) \\cdot g_\\alpha^\\ell(t) \\rangle $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continual Learning and Parametrisations\n",
    "\n",
    "### NTP\n",
    "Applying the same derivations to the Continual Learning framework, thus looking at the evolution of residuals of Task 1 while we are training Task 2, we can obtain something of the same fashion of above for the NTP case:\n",
    "####  $$ \\frac{\\partial \\Delta_{\\mu_1}(t)}{\\partial t} = - K^{NTK}_{\\mu_1 \\alpha_2} \\Delta_{\\alpha_2}(t) $$  \n",
    "So the new object $K^{NTK}_{\\mu_1 \\alpha_2}$, called NTK Across Tasks, gives crucial information about the residuals evolution, thus it allows us to obtain the loss evolution during other tasks' training\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"lazy_regime/residuals_ntk.png\" alt=\"ntp_residuals\" width=\"1000\"/>\n",
    "<p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\mu P$\n",
    "\n",
    "In this case we need the updates equations for the fields $h_\\mu^\\ell(t)$ and $g_\\mu^\\ell(t)$ to obtain the evolution of the NTK and thus the residuals evolution. This can be made applying the gradient descent dynamics in the recursive relation that defined both the forward and backward field, obtaining the desired equations:\n",
    "\n",
    "$$ \\frac{d}{dt}\\boldsymbol{h}_{\\mathcal{T}_1}^1(t) =  \\gamma_0 \\left[ \\Theta(t_1-t) \\boldsymbol{\\Delta}_{\\mathcal{T}_1}(t) \\boldsymbol{g}_{{\\mathcal{T}_1}}^{1}(t) K^x_{\\mathcal{T}_1 \\mathcal{T}_1}  + \\Theta (t-t_1)\\boldsymbol{\\Delta}_{{\\mathcal{T}_2}}(t) \\boldsymbol{g}_{\\mathcal{T}_2}^{1}(t) K^x_{\\mathcal{T}_1 \\mathcal{T}_2} \\right]  $$\n",
    "\n",
    "$$ \\frac{d}{dt}\\boldsymbol{z}^1_{\\mathcal{T}_1}(t) =  \\gamma_0 \\left[ \\Theta(t_1-t) \n",
    "    \\boldsymbol{\\Delta}_{\\mathcal{T}_1}(t) \\phi(\\boldsymbol{h}_{\\mathcal{T}_1}^1(t))   + \n",
    "    \\Theta (t-t_1)\n",
    "    \\boldsymbol{\\Delta}_{\\mathcal{T}_2}(t) \\phi(\\boldsymbol{h}_{\\mathcal{T}_2}^1(t))\\right] $$ \n",
    "\n",
    "$$ \\frac{\\partial}{\\partial t} \n",
    "    \\boldsymbol{\\Delta}_{\\mathcal{T}_1}(t)  = -[\\Phi^1_{\\mathcal{T}_1\\mathcal{T}_2}(t) + G^1_{\\mathcal{T}_1\\mathcal{T}_2}(t) \\boldsymbol{K}^x_{\\mathcal{T}_1 \\mathcal{T}_2} ] \\boldsymbol{\\Delta}_{\\mathcal{T}_2}(t)  $$\n",
    "\n",
    "We can thus obtain the residuals evolution and the internal representation evolution of both fields:\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"rich_regime/h_dist_mup_1.png\" alt=h_dist\" width=\"1.8*350\" height=\"350\"/>\n",
    "<img src=\"rich_regime/z_dist_mup_1.png\" alt=\"z_dist\" width=\"1.8*350\" height=\"350\"/>\n",
    "<p>\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"rich_regime/phi_mup_1.png\" alt=\"phi_evol\" width=\"1.8*300\" height=\"270\"/>\n",
    "<img src=\"rich_regime/g_mup_1.png\" alt=\"g_evol\" width=\"1.8*300\" height=\"270\"/>\n",
    "<p>\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"rich_regime/loss_mup_1.png\" alt=\"loss_evol\" width=\"1.8*350\" height=\"400\"/>\n",
    "<p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forgetting\n",
    "\n",
    "Once we know which are the quantities involved in the evolution of the all tasks losses, we can try to address the source of forgetting and identify what we can do to cope with that. \n",
    "We have defined forgetting on task 1 as negative backward transfer, that is defined as:\n",
    "#### $$ BWT(t) = \\mathcal{L}(t_1) - \\mathcal{L}(t) $$\n",
    "\n",
    "If we look at time evolution of the quantity above, and impose it equal to 0, we get that the features of the forward and backward fields of different tasks must be orhogonal to each others:\n",
    "#### $$ \\langle \\phi(h^\\ell_{\\mu_1}(t)) \\cdot \\phi(h^\\ell_{\\alpha_2}(t)) \\rangle = 0, \\quad \\langle g^\\ell_{\\mu_1}(t) \\cdot g^\\ell_{\\alpha_2}(t) \\rangle = 0 $$\n",
    "We are still working to have a looser characterization of the condition on the non-negative backward transfer..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity\n",
    "We also imposed a couple of assumptions on the input covariance matrix of tasks and also between tasks: the $K^x$ of each task will be the identity, $K^x_{ii} = \\mathbb{I}$ while the $K^x$ across tasks will be $K^x= \\epsilon \\mathbb{I}$.\n",
    "\n",
    "We are trying to characterize how $\\epsilon$ and $\\gamma_0$ impact together the forgetting. We have simulation based on the DMFT equations that are perfectly aligned with the empirical results, leading some concave shape in the forgetting vs similarity plots\n",
    "\n",
    "## Forgetting\n",
    "Top curve: Linear\n",
    "\n",
    "Bottom curve: Relu\n",
    "<p align=\"center\">\n",
    "<img src=\"forgetting/forg_vs_eps.png\" alt=\"forg_evol_relu\"  height=\"450\"/>\n",
    "<p>\n",
    "\n",
    "## Normalized Forgetting\n",
    "<p align=\"center\">\n",
    "<img src=\"forgetting/forg_vs_eps_normalized.png\" alt=\"forg_evol_relu\"  height=\"450\"/>\n",
    "<p>\n",
    "\n",
    "## Forgetting\n",
    "<p align=\"center\">\n",
    "<img src=\"forgetting/gamma0_vs_eps.png\" alt=\"forg_evol_relu\"  height=\"550\"/>\n",
    "<p>\n",
    "\n",
    "## Normalized Forgetting\n",
    "<p align=\"center\">\n",
    "<img src=\"forgetting/gamma0_vs_eps_normalized.png\" alt=\"forg_evol_relu\"  height=\"550\"/>\n",
    "<p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
